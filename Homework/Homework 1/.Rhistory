win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 4
m <- 0
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 4
m <- 1
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 4
m <- 2
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 5
m <- 0
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 5
m <- 1
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 5
m <- 2
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
## Including libraries
remove(list=objects())
## Including libraries
remove(list=objects())
library(grid)
## Including libraries
##remove(list=objects())
library(grid)
install.packages("plotrix")
## Including libraries
remove(list=objects())
library(grid)
library(lattice)
library(plotrix)
dat1<-rnorm(2000)
qqnorm(dat1);
qqline(dat1,col=2)
dat2<-rexp(2000)
qqnorm(dat2);
qqline(dat2,col=2)
dat3<-rpois(2000,200)
qqnorm(dat3);
qqline(dat3,col=2)
hist(dat3,breaks=100)
dat4<-rt(1000,2)
#2*((runif(1000)>0.5)-0.5)*(runif(1000))^(-0.5)
qqnorm(dat4,ylim=c(-10,10));
qqline(dat4,col=2)
library(plotrix)
datavalues <- c(6.10, 6.74, 6.22, 5.65, 6.38, 6.70, 7.00, 6.43, 7.00, 6.70, 6.70, 5.94, 6.28, 6.34, 6.62, 6.55, 2.92, 6.10, 6.20, 6.70, 7.00, 6.85, 6.31, 6.26, 6.36, 6.28, 6.38, 6.70, 6.62, 7.00, 6.45, 6.31, 2.86, 6.31, 6.09, 6.17, 6.64, 6.45, 7.00, 6.18, 6.58, 5.38, 6.34, 7.00, 5.70, 6.65, 6.56, 6.00, 6.70, 6.45)
summary(datavalues)
mean(datavalues, trim=0.1)
sd(datavalues)
boxplot(datavalues)
qqnorm(datavalues)
qqline(datavalues, col=2)
library(grid)
library(lattice)
library(plotrix)
data6 <- c(20, 18, 25, 25, 0, 20, 60, 0, 20, 10, 10, 20, 40, 26, 40, 12, 16, 16, 36, 38, 21, 15, 13, 10, 10)
summary(data6)
sd(data6)
qqnorm(data6)
qqline(data6, col=2)
install.packages("IIS")
show(pines_1997)
data(pines_1997)
help(data)
data(package="IIS")
data("pines_1997")
data(pines_1997)
data("gender_roles")
install.packages(IIS)
library(IIS)
install.packages("IIS")
data("pines_1997")
install.packages(c("asbio", "bayesplot", "BDgraph", "BH", "bit", "blavaan", "blob", "boot", "broom", "callr", "caTools", "checkmate", "cli", "curl", "data.table", "DBI", "dendextend", "DescTools", "deSolve", "digest", "dplyr", "DT", "emmeans", "fansi", "forcats", "foreach", "foreign", "future", "future.apply", "ggm", "ggplot2", "ggridges", "globals", "gplots", "Hmisc", "hms", "htmlTable", "huge", "igraph", "jpeg", "jsonlite", "KernSmooth", "knitr", "lattice", "latticeExtra", "lifecycle", "listenv", "lmerTest", "loo", "MASS", "Matrix", "mcmc", "MCMCpack", "mgcv", "mime", "mnormt", "modeltools", "MuMIn", "mvtnorm", "nlme", "nloptr", "nnet", "pillar", "plyr", "prettyunits", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantreg", "R6", "Rcpp", "RcppEigen", "rlang", "rmarkdown", "rsconnect", "RSQLite", "rstan", "rstanarm", "rstudioapi", "scales", "shinyjs", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tidyr", "tidyselect", "tinytex", "TSP", "vctrs", "xfun", "xts", "yaml", "zoo"))
install.packages(c("asbio", "bayesplot", "BDgraph", "BH", "bit", "blavaan", "blob", "boot", "broom", "callr", "caTools", "checkmate", "cli", "curl", "data.table", "DBI", "dendextend", "DescTools", "deSolve", "digest", "dplyr", "DT", "emmeans", "fansi", "forcats", "foreach", "foreign", "future", "future.apply", "ggm", "ggplot2", "ggridges", "globals", "gplots", "Hmisc", "hms", "htmlTable", "huge", "igraph", "jpeg", "jsonlite", "KernSmooth", "knitr", "lattice", "latticeExtra", "lifecycle", "listenv", "lmerTest", "loo", "MASS", "Matrix", "mcmc", "MCMCpack", "mgcv", "mime", "mnormt", "modeltools", "MuMIn", "mvtnorm", "nlme", "nloptr", "nnet", "pillar", "plyr", "prettyunits", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantreg", "R6", "Rcpp", "RcppEigen", "rlang", "rmarkdown", "rsconnect", "RSQLite", "rstan", "rstanarm", "rstudioapi", "scales", "shinyjs", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tidyr", "tidyselect", "tinytex", "TSP", "vctrs", "xfun", "xts", "yaml", "zoo"))
install.packages(c("asbio", "bayesplot", "BDgraph", "BH", "bit", "blavaan", "blob", "boot", "broom", "callr", "caTools", "checkmate", "cli", "curl", "data.table", "DBI", "dendextend", "DescTools", "deSolve", "digest", "dplyr", "DT", "emmeans", "fansi", "forcats", "foreach", "foreign", "future", "future.apply", "ggm", "ggplot2", "ggridges", "globals", "gplots", "Hmisc", "hms", "htmlTable", "huge", "igraph", "jpeg", "jsonlite", "KernSmooth", "knitr", "lattice", "latticeExtra", "lifecycle", "listenv", "lmerTest", "loo", "MASS", "Matrix", "mcmc", "MCMCpack", "mgcv", "mime", "mnormt", "modeltools", "MuMIn", "mvtnorm", "nlme", "nloptr", "nnet", "pillar", "plyr", "prettyunits", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantreg", "R6", "Rcpp", "RcppEigen", "rlang", "rmarkdown", "rsconnect", "RSQLite", "rstan", "rstanarm", "rstudioapi", "scales", "shinyjs", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tidyr", "tidyselect", "tinytex", "TSP", "vctrs", "xfun", "xts", "yaml", "zoo"))
install.packages(c("asbio", "bayesplot", "BDgraph", "BH", "bit", "blavaan", "blob", "boot", "broom", "callr", "caTools", "checkmate", "cli", "curl", "data.table", "DBI", "dendextend", "DescTools", "deSolve", "digest", "dplyr", "DT", "emmeans", "fansi", "forcats", "foreach", "foreign", "future", "future.apply", "ggm", "ggplot2", "ggridges", "globals", "gplots", "Hmisc", "hms", "htmlTable", "huge", "igraph", "jpeg", "jsonlite", "KernSmooth", "knitr", "lattice", "latticeExtra", "lifecycle", "listenv", "lmerTest", "loo", "MASS", "Matrix", "mcmc", "MCMCpack", "mgcv", "mime", "mnormt", "modeltools", "MuMIn", "mvtnorm", "nlme", "nloptr", "nnet", "pillar", "plyr", "prettyunits", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantreg", "R6", "Rcpp", "RcppEigen", "rlang", "rmarkdown", "rsconnect", "RSQLite", "rstan", "rstanarm", "rstudioapi", "scales", "shinyjs", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tidyr", "tidyselect", "tinytex", "TSP", "vctrs", "xfun", "xts", "yaml", "zoo"))
install.packages(c("asbio", "bayesplot", "BDgraph", "BH", "bit", "blavaan", "blob", "boot", "broom", "callr", "caTools", "checkmate", "cli", "curl", "data.table", "DBI", "dendextend", "DescTools", "deSolve", "digest", "dplyr", "DT", "emmeans", "fansi", "forcats", "foreach", "foreign", "future", "future.apply", "ggm", "ggplot2", "ggridges", "globals", "gplots", "Hmisc", "hms", "htmlTable", "huge", "igraph", "jpeg", "jsonlite", "KernSmooth", "knitr", "lattice", "latticeExtra", "lifecycle", "listenv", "lmerTest", "loo", "MASS", "Matrix", "mcmc", "MCMCpack", "mgcv", "mime", "mnormt", "modeltools", "MuMIn", "mvtnorm", "nlme", "nloptr", "nnet", "pillar", "plyr", "prettyunits", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantreg", "R6", "Rcpp", "RcppEigen", "rlang", "rmarkdown", "rsconnect", "RSQLite", "rstan", "rstanarm", "rstudioapi", "scales", "shinyjs", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tidyr", "tidyselect", "tinytex", "TSP", "vctrs", "xfun", "xts", "yaml", "zoo"))
help(package)
help(data)
installed.packages()
install.packages(IIT-package)
install.packages(IIS-package)
install.packages("IIS-package")
install.packages("IIS")
installed.packages()
data("IIS")
data("pines_1997")
library(IIS)
install.packages("Stat2Data")
library(Stat2Data)
data(pines)
data("pines")
data(Pines)
summary(Pines)
install.packages(IIS-package)
load("/Users/yingxuan/Downloads/IIS/data/pines_1997.rda")
View(pines_1997)
install.packages("~/Downloads/IIS_1.0.tar.gz", repos = NULL, type = "source")
a <- 1:10/5
print(a)
a = 1:(10/5)
print (a)
for (i in 1:5) {b <- 2^i; print(b)}
install.packages("shiny")
1-pchisq(2(7-1), 7)
1-pchisq(2*(7-1), 7)
1-pchisq(2*(7-1), 6)
1-pchisq(2*(8-1), 6)
1-pchisq(2*(8-1), 7)
1-pchisq(2*(17-1), 16)
1-pchisq(2*20, 20)
qt(0.95, df=99)
qt(0.95, df=100)
qt(0.9725, df=99)
qt(0.9725, df=100)
qt(0.025, df=99)
qt(p=(1-0.025), df=99)
qt(p=0.05, df=99)
qt(p=(1-0.05), df=99)
qt(p=(1-0.025), df=99)
qt(p=(1-0.025), df=9)
qchisq(p=0.05, 99)
qchisq(p=0.95, df=99)
qchisq(p=0.95, df=99)
qnorm(0.95)
qnorm(0.975)
qt(p=0.95, df=538)
qt(p=0.975, df=538)
qt(p=0.95, df=539)
qnorm(0.95)
remove(list = ls())
qqline(data, col=2)
library(grid)
library(lattice)
library(plotrix)
data <- c(24.00, 28.00, 27.75, 27.00, 24.25, 23.50, 26.25, 24.00, 25.00, 30.00,23.25, 26.25, 21.50, 26.00, 28.00, 24.50, 22.50, 28.25, 21.25, 19.75)
qqnorm(data)
qqline(data, col=2)
mean(data)
var(data)
qt(p=0.0975, df=19)
qt(p=0.975, df=19)
qt(p=0.975, df=99)
dataX <- c(12.0129, 12.0072, 12.0064, 12.0054, 12.0016,11.9853, 11.9949, 11.9985, 12.0077, 12.0061)
dataY <- c(12.0318, 12.0246, 12.0069, 12.0006, 12.0075)
mean(dataX)
mean(dataY)
var(dataX)
var(dataY)
qt(p=0.975, df=23)
qt(p=0.95, df=23)
qt(p=0.975, df=13)
qt(p=0.975, df=4)
qt(p=0.975, df=4.48)
qt(p=0.975, df=5.48)
qt(p=0.975, df=5)
qf(p=0.95, df1 = 8, df2= 8)
## Including libraries
remove(list=objects())
## Including libraries
remove(list=objects())
####### Lecture 2-2
## Facebook friends
facfr<-c(108, 103, 352, 121, 93, 53, 40, 53, 22, 116, 94)
facor<-facfr[order(facfr,decreasing=F)]
mean(facor)
median(facor)
facornew<-facor[-11]
mean(facornew)
median(facornew)
summary(facor)
### Morley data set
speed<-morley[,3]
mean(speed)
sd(speed)
summary(speed)
boxplot(speed)
hist(speed)
hist(speed, breaks=20)
### qq plot
qqnorm(speed)
qqline(speed,col=2)
dat1<-rnorm(2000)
qqnorm(dat1);
qqline(dat1,col=2)
dat2<-rexp(200)
qqnorm(dat2);
qqline(dat2,col=2)
dat3<-rpois(2000,200)
qqnorm(dat3);
qqline(dat3,col=2)
dat4<-2*((runif(1000)>0.5)-0.5)*(runif(1000))^(-0.5)
qqnorm(dat4,ylim=c(-10,10));
qqline(dat4,col=2)
### scatter plot: 31 Black cherry trees. girth (diameter), height,  volume
plot(trees[,1],trees[,2])
splom(trees)
### 5 Orange trees circumference at age
plot(Orange[,2],Orange[,3])
xyplot(circumference~age,Orange,groups= Tree,cex=1,pch=c(1,2,3,4,5))
### Anscombe look for help(anscombe)
summary(anscombe)
boxplot(anscombe)
##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
for(i in 1:4) {
ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
## or   ff[[2]] <- as.name(paste0("y", i))
##      ff[[3]] <- as.name(paste0("x", i))
mods[[i]] <- lmi <- lm(ff, data = anscombe)
print(anova(lmi))
}
## See how close they are (numerically!)
sapply(mods, coef)
lapply(mods, function(fm) coef(summary(fm)))
## Now, do what you should have done in the first place: PLOTS
op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))
for(i in 1:4) {
ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
xlim = c(3, 19), ylim = c(3, 13))
abline(mods[[i]], col = "blue")
}
mtext("Anscombe's 4 Regression data sets", outer = TRUE, cex = 1.5)
par(op)
test <- c(1, 2, 3)
test
as.character(test)
# Remove all variables from the R environment to create a fresh start
rm(list=ls())
# Set the working folder -- FILL IN THE LINE BELOW
setwd("~/Documents/SUTD/Term 5/ESA/Week 9/Lecture 15")
# Import data from EngineDesign.csv
mydata <- read.csv(file="EngineDesign.csv", head=TRUE)
# Determine the 'best' solution according to the Lexicographic method
# -> we simply have to look for the minimum (or maximum) value of each objective -- FILL IN THE LINES BELOW
# Horsepower (to be maximized)
max(mydata[,1]) # 0.679458
which.max(mydata[,1]) # 10
# Cost (to be minimized)
min(mydata[,2]) # 0.014658
which.min(mydata[,2]) # 43
# Fuel.efficiency (to be maximized)
max(mydata[,3]) # 0.74487
which.max(mydata[,3]) # 4
# First, we identify the Pareto-efficient solutions (we want to carry out the analysis only for the non-dominated solution)
input <- matrix(c(mydata$Horsepower,mydata$Cost*(-1),mydata$Fuel.efficiency),nrow=43,ncol=3)
source("ParetoSorting_adv.R")
result <- ParetoSorting_adv(input,"MAX") # We use the ParetoSorting_adv function to identify the non-dominated solutions (denoted with 0)
index <- which(result==0) # We store in a vector the index of the Pareto-efficient solutions
# Second, we find the coordinates of the Ideal (Utopia) and Nadir objective vector
# (we look for the minimum and maximum value of the objective functions only for the efficient solutions)
Utopia <- c(max(input[index,1]),max(input[index,2]),max(input[index,3])) # 0.679458 -0.014658  0.744870
Nadir  <- c(min(input[index,1]),min(input[index,2]),min(input[index,3])) # 0.348350 -0.162701  0.709748
# Third, we normalize the data (since the objectives have a different range of variation). Let's normalize only the Pareto-efficient solutions -- FILL IN THE LINES BELOW
obj1_norm <- (input[index,1]-min(input[index,1]))/(max(input[index,1])-min(input[index,1]))
obj2_norm <- (input[index,2]-min(input[index,2]))/(max(input[index,2])-min(input[index,2]))
obj3_norm <- (input[index,3]-min(input[index,3]))/(max(input[index,3])-min(input[index,3]))
# The coordinates of the Utopia point for the normalised data are 1 1 1. Let's double-check this ...
Utopia_norm <- c(max(obj1_norm),max(obj2_norm),max(obj3_norm)) # 1 1 1
# We calculate the distance between each solution and the Utopia point, and we find the point with the minimum distance
# We have to do this only for the Pareto-efficient solutions -- FILL IN THE LINES BELOW
Distance <- rep(0,times=40)
for (i in 1:40){
Distance[i] <- sqrt((obj1_norm[i] - Utopia_norm[1])^2 + (obj2_norm[i] - Utopia_norm[2])^2 + (obj3_norm[i] - Utopia_norm[3])^2)
}
plot(Distance)
#
# Create data frames for the visualization
newdata <- data.frame(Horsepower=obj1_norm,Cost=obj2_norm,Fuel.efficiency=obj3_norm)
newdata_utopia <- data.frame(Horsepower=1,Cost=1,Fuel.efficiency=1)
newdata_sel_point <- data.frame(Horsepower=obj1_norm[28],Cost=obj2_norm[28],Fuel.efficiency=obj3_norm[28])
newdata$cat <- "Pareto-eff. sol."
View(newdata)
View(mydata)
View(newdata_utopia)
#Homework 1
rm(list=ls())
setwd("~/Documents/SUTD/Term 5/ESA/Homework/Homework 1")
library(ggplot2)
library(GGally)
dataset <- read.csv(file="Tour_de_France.csv")
# Scatter Plot 1: Fatigue against Crashes
t1_plot1 <- ggplot(data = dataset, mapping = aes(x = Fatigue, y = Crashes)) +
geom_point() +
labs(title = "Task 1 Scatter Plot 1: Fatigue vs Crashes", x = "Fatigue", y = "Crashes", caption = "Tour_de_France.csv") +
geom_smooth(method = 'lm', color = 'blue')
t1_plot1
if(!require(ggplot2)){
install.packages("ggplot2")
library(ggplot2)
}
if(!require(GGally)) {
install.packages("GGally")
library(GGally)
}
# Scatter Plot 1: Fatigue against Crashes
t1_plot1 <- ggplot(data = dataset, mapping = aes(x = Fatigue, y = Crashes)) +
geom_point() +
labs(title = "Task 1 Scatter Plot 1: Fatigue vs Crashes", x = "Fatigue", y = "Crashes", caption = "Tour_de_France.csv") +
# geom_smooth(method = 'lm', color = 'blue') +
geom_smooth()
t1_plot1
?geom_smooth
# Task 9
new_norm_data <- data.frame(Fatigue = fatigue_norm, Crashes = crashes_norm, Disruptions = disruptions_norm)
#Homework 1
rm(list=ls())
setwd("~/Documents/SUTD/Term 5/ESA/Homework/Homework 1")
if(!require(ggplot2)){
install.packages("ggplot2")
library(ggplot2)
}
if(!require(GGally)) {
install.packages("GGally")
library(GGally)
}
dataset <- read.csv(file="Tour_de_France.csv")
# Scatter Plot 1: Fatigue against Crashes
t1_plot1 <- ggplot(data = dataset, mapping = aes(x = Fatigue, y = Crashes)) +
geom_point() +
labs(title = "Task 1 Scatter Plot 1: Fatigue vs Crashes", x = "Fatigue", y = "Crashes", caption = "Tour_de_France.csv") +
# geom_smooth(method = 'lm', color = 'blue') +
geom_smooth(method = 'lm', color = "blue")
t1_plot1
# Scatter Plot 2: Fatigue against Disruptions
t1_plot2 <- ggplot(data = dataset, mapping = aes(x = Fatigue, y = Disruptions)) +
geom_point() +
labs(title = "Task 1 Scatter Plot 2: Fatigue vs Disruptions", x = "Fatigue", y = "Disruptions", caption = "Tour_de_France.csv") +
geom_smooth(method = 'lm', color = "blue")
t1_plot2
# Scatter Plot 3: Crashes against Disruptions
t1_plot3 <- ggplot(data = dataset, mapping = aes(x = Crashes, y = Disruptions)) +
geom_point() +
labs(title = "Task 1 Scatter Plot 3: Crashes vs Disruptions", x = "Crashes", y = "Disruptions", caption = "Tour_de_France.csv") +
geom_smooth(method = 'lm', color = "blue")
t1_plot3
# Task 3
# Plot: Fatigue vs Crashes, Disruptions (colour)
t3_plot1 <- ggplot(data = dataset, mapping = aes(x = Fatigue, y = Crashes, colour = Disruptions)) +
geom_point() +
labs(title = "Task 3 Scatter Plot: Fatigue (x-axis) vs Crashes (y-axis) vs Disruptions (colour)", x = "Fatigue", y = "Crashes", caption = "Tour_de_France.csv")
t3_plot1
#Task 5
source("ParetoSorting_adv.R")
# data in matrix form to input into pareto sorting function
# N x M matrix with N alternatives (rows) and M objectives (columns)
matrix_input <- matrix(c(dataset$Fatigue, dataset$Crashes, dataset$Disruptions), nrow = nrow(dataset), ncol = ncol(dataset))
result <- ParetoSorting_adv(matrix_input, "MIN")
result
# dominated count
dominated_count <- length(which(result == 1))
dominated_count
# not dominated count
notdominated_count <- length(which(result == 0))
notdominated_count
# function to create a list stating string "dominated" or "non-dominated"
dom_func <- function(list_input) {
result_list <- c()
for (i in 1:length(list_input)) {
if (list_input[i] == 1) {
name <- "Dominated"
} else {
name <- "Non-dominated"
}
result_list <- c(result_list, name)
}
return(result_list)
}
dom_list <- dom_func(result)
dataset$Dominance <- dom_list
domplot <- ggplot(data = dataset, mapping = aes(x=Fatigue, y = Crashes)) +
geom_point(aes(color = Dominance)) +
labs(title = "Domination Plot", caption ="Tour_de_France.csv")
# Task 6
t6_plot1 <- ggparcoord(data = data.frame(Fatigue = dataset[,1], Crashes = dataset[,2], Disruptions = dataset[,3], Domination = as.factor(result)),
columns = 1:3,
title = "Task 6 Plot 1: Parallel Plot",
scale = "uniminmax",
groupColumn = "Domination")
t6_plot1
# Task 7
# list of non-dominated solutions
index <- which(result==0)
# Utopia vector
utopia_fatigue <- min(dataset[index, 1])
utopia_crashes <- min(dataset[index, 2])
utopia_disruptions <- min(dataset[index, 3])
utopia_vector <- c(utopia_fatigue, utopia_crashes, utopia_disruptions)
# Nadir vector
nadir_fatigue <- max(dataset[index, 1])
nadir_crashes <- max(dataset[index, 2])
nadir_disruptions <- max(dataset[index, 3])
nadir_vector <- c(nadir_fatigue, nadir_crashes, nadir_disruptions)
# Task 8
# normalise non-dominated set of solutions
fatigue_norm <- (dataset[index, 1] - utopia_fatigue) / (nadir_fatigue - utopia_fatigue)
crashes_norm <- (dataset[index, 2] - utopia_crashes) / (nadir_crashes - utopia_crashes)
disruptions_norm <- (dataset[index, 3] - utopia_disruptions) / (nadir_disruptions - utopia_disruptions)
# check normality
utopia_norm <- c(min(fatigue_norm), min(crashes_norm), min(disruptions_norm))
distance <- rep(0, times = notdominated_count)
for (i in 1:notdominated_count) {
distance[i] <- sqrt((fatigue_norm[i] - utopia_norm[1])^2 + (crashes_norm[i] - utopia_norm[2])^2 + (disruptions_norm[i] - utopia_norm[3])^2)
}
min_dist <- min(distance)
# min_dist 0.3555774
index_min_dist <- which.min(distance)
# 403th row value in the set of non-dominated solutions
index[403]
# Task 9
new_norm_data <- data.frame(Fatigue = fatigue_norm, Crashes = crashes_norm, Disruptions = disruptions_norm)
utopia <- data.frame(Fatigue = 0, Crashes = 0, Disruptions = 0)
selected <- data.frame(Fatigue = fatigue_norm[403], Crashes = crashes_norm[403], Disruptions = disruptions_norm[403])
new_norm_data
new_norm_data$cat <- "Pareto-efficient Solution"
utopia$cat <- "Utopia Point"
selected$cat <- "Selected Solution"
utopia
df <- rbind(new_norm_data, utopia, selected)
df
t9_plot1 <- ggplot(data = df, mapping = aes(x = Fatigue, y = Crashes, size = Disruptions)) +
geom_point(aes(color=cat)) +
labs(title = "Task 9 Plot 1", x = "Fatigue", y = "Crashes")
t9_plot1
if(!require(fmsb)) {
install.packages("fmsb")
library(fmsb)
}
index
selected
selected_point <- c(selected[, 1:3])
selected_point
max
# Task 10
max_val <- c(1, 1, 1)
min_val <- c(0, 0, 0)
max_val
min_val
selected_point <- c(selected[1, 1], selected[1, 2], selected[1, 3])
selected_point
radar_data <- rbind(max_val, min_val, selected_point)
dataset[1065]
dataset[1065,]
radar_data <- rbind(max_val, min_val, selected_point)
t10_plot1 <- radarchart(df = radar_data, title = "Task 10 Plot 1: Best Alternative")
legend(1, 1, lty = c(1, 2), lwd = c(2.5, 2.5), col = c("black"), c("Best Alternative"), cex = 0.4)
t10_plot1 <- radarchart(df = radar_data, title = "Task 10 Plot 1: Best Alternative")
radar_data <- rbind(max_val, min_val, selected_point)
t10_plot1 <- radarchart(df = radar_data, title = "Task 10 Plot 1: Best Alternative")
selected
selected_point <- selected[, 1:3]
radar_data <- rbind(max_val, min_val, selected[, 1:3])
t10_plot1 <- radarchart(df = radar_data, title = "Task 10 Plot 1: Best Alternative")
legend(1, 1, lty = c(1, 2), lwd = c(2.5, 2.5), col = c("black"), c("Best Alternative"), cex = 0.4)
selected[, 1:3]
View(dataset)
