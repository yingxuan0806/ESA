# Capture the name of the database file (we assume it is located in the working directory)
dbname <- "NewExtract.sqlite"
# Establish a database connection to this database file
conn <- dbConnect(SQLite(),dbname)
# Take a look at the structure of this connection: it should display the defaults, such as loadable.extensions=TRUE
str(conn)
# Assuming there is no error in connecting, we try now to get some data from the database.
# Define an SQL query to get a small amount of data (use SQLite syntax)
query <- "SELECT * FROM NewExtract LIMIT 20"
# Use the connection to get the query results
queryresult <- dbGetQuery(conn,query)
# Close the connection to the database
dbDisconnect(conn)
# Return the result (it is in the form of a data.frame)
queryresult
}
# Run the next statement (without the comment #) if you want a quick test
#testRSQLite()
#### Execute SQL queries in R
# Let's create a function to grab data from our database according to a query we provide. This will be useful.
getExtract <- function(query){
# Capture the name of the database file (we assume it is located in the working directory)
dbname <- "NewExtract.sqlite"
# Establish a database connection to this database file
conn <- dbConnect(SQLite(),dbname)
# Use the connection to get the query results
queryresult <- dbGetQuery(conn,query)
# Close the connection to the database
dbDisconnect(conn)
# Return the result (it is in the form of a data.frame)
queryresult
}
# Run the next statement to test the getExtract function
getExtract("SELECT * FROM NewExtract LIMIT 20")
#Execute Statements One at a Time
# Define a query to aggregate actual sales by region
query <- "select region,sum(actual) as TotalActual from NewExtract group by region order by sum(actual) desc"
# Grab the data into a data.frame
result <- getExtract(query)
# Display the result
result
# Plot the values
plot(result$TotalActual)
# Plot the values as a bar chart
barplot(result$TotalActual)
# Plot the values as a bar chart with labels
barplot(result$TotalActual,names.arg=result$Region)
# Plot the values as a treemap with labels
treemap(result,title="Total Quantity by Region",index="Region",vSize="TotalActual")
# Which plot pattern do you like best?
install.packages("stats")
n <- 5   #points player B needs to win
clear()
rm()
rm(list = ls())
n <- 5  #points player B needs to win
m <- 5  #points player A needs to win
margin <- 2
p <- 0.5  #probability that A wins a point
n.sim <- 1e4   #number of iterations in this simulation
sim <- replicate((n.sim {}))
sim <- replicate(n.sim, {
x <- sample(1:0, 3 * (m + n), prob = c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
})
sim <- replicate(n.sim, {
x <- sample(1:0, 3 * (m+n), prob = c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
mean(sim)
n <- 21
m <- 21
p <- 0.58
mean(sim)
sim <- replicate(n.sim, {
+     x <- sample(1:0, 3 * (m+n), prob = c(p, 1-p), replace = TRUE)
+     points.1 <- cumsum(x)
+     points.0 <- cumsum(1-x)
+     win.1 <- points.1 >= m & points.0 <= points.1 - margin
+     win.0 <- points.0 >= n & points.1 <= points.0 - margin
+     which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
+ })
n <- 21
m <- 21
margin <- 2
p <- 0.58
n.sim <- 1e4
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob = c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 > n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) > which.max(c(win.0), TRUE))
})
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
mean(sim)
n <- 5
m <- 5
mean(sim)
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 3
m <- 3
p <- 0.5
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 2
m <- 0
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 3
m <- 1
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 3
m <- 0
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 4
m <- 0
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 4
m <- 1
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 4
m <- 2
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 5
m <- 0
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 5
m <- 1
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
mean(sim)
n <- 5
m <- 2
sim <- replicate(n.sim, {
x <- sample(1:0, 3*(m+n), prob=c(p, 1-p), replace = TRUE)
points.1 <- cumsum(x)
points.0 <- cumsum(1-x)
win.1 <- points.1 >= m & points.0 <= points.1 - margin
win.0 <- points.0 >= n & points.1 <= points.0 - margin
which.max(c(win.1, TRUE)) < which.max(c(win.0, TRUE))
})
## Including libraries
remove(list=objects())
## Including libraries
remove(list=objects())
library(grid)
## Including libraries
##remove(list=objects())
library(grid)
install.packages("plotrix")
## Including libraries
remove(list=objects())
library(grid)
library(lattice)
library(plotrix)
dat1<-rnorm(2000)
qqnorm(dat1);
qqline(dat1,col=2)
dat2<-rexp(2000)
qqnorm(dat2);
qqline(dat2,col=2)
dat3<-rpois(2000,200)
qqnorm(dat3);
qqline(dat3,col=2)
hist(dat3,breaks=100)
dat4<-rt(1000,2)
#2*((runif(1000)>0.5)-0.5)*(runif(1000))^(-0.5)
qqnorm(dat4,ylim=c(-10,10));
qqline(dat4,col=2)
library(plotrix)
datavalues <- c(6.10, 6.74, 6.22, 5.65, 6.38, 6.70, 7.00, 6.43, 7.00, 6.70, 6.70, 5.94, 6.28, 6.34, 6.62, 6.55, 2.92, 6.10, 6.20, 6.70, 7.00, 6.85, 6.31, 6.26, 6.36, 6.28, 6.38, 6.70, 6.62, 7.00, 6.45, 6.31, 2.86, 6.31, 6.09, 6.17, 6.64, 6.45, 7.00, 6.18, 6.58, 5.38, 6.34, 7.00, 5.70, 6.65, 6.56, 6.00, 6.70, 6.45)
summary(datavalues)
mean(datavalues, trim=0.1)
sd(datavalues)
boxplot(datavalues)
qqnorm(datavalues)
qqline(datavalues, col=2)
library(grid)
library(lattice)
library(plotrix)
data6 <- c(20, 18, 25, 25, 0, 20, 60, 0, 20, 10, 10, 20, 40, 26, 40, 12, 16, 16, 36, 38, 21, 15, 13, 10, 10)
summary(data6)
sd(data6)
qqnorm(data6)
qqline(data6, col=2)
install.packages("IIS")
show(pines_1997)
data(pines_1997)
help(data)
data(package="IIS")
data("pines_1997")
data(pines_1997)
data("gender_roles")
install.packages(IIS)
library(IIS)
install.packages("IIS")
data("pines_1997")
install.packages(c("asbio", "bayesplot", "BDgraph", "BH", "bit", "blavaan", "blob", "boot", "broom", "callr", "caTools", "checkmate", "cli", "curl", "data.table", "DBI", "dendextend", "DescTools", "deSolve", "digest", "dplyr", "DT", "emmeans", "fansi", "forcats", "foreach", "foreign", "future", "future.apply", "ggm", "ggplot2", "ggridges", "globals", "gplots", "Hmisc", "hms", "htmlTable", "huge", "igraph", "jpeg", "jsonlite", "KernSmooth", "knitr", "lattice", "latticeExtra", "lifecycle", "listenv", "lmerTest", "loo", "MASS", "Matrix", "mcmc", "MCMCpack", "mgcv", "mime", "mnormt", "modeltools", "MuMIn", "mvtnorm", "nlme", "nloptr", "nnet", "pillar", "plyr", "prettyunits", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantreg", "R6", "Rcpp", "RcppEigen", "rlang", "rmarkdown", "rsconnect", "RSQLite", "rstan", "rstanarm", "rstudioapi", "scales", "shinyjs", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tidyr", "tidyselect", "tinytex", "TSP", "vctrs", "xfun", "xts", "yaml", "zoo"))
install.packages(c("asbio", "bayesplot", "BDgraph", "BH", "bit", "blavaan", "blob", "boot", "broom", "callr", "caTools", "checkmate", "cli", "curl", "data.table", "DBI", "dendextend", "DescTools", "deSolve", "digest", "dplyr", "DT", "emmeans", "fansi", "forcats", "foreach", "foreign", "future", "future.apply", "ggm", "ggplot2", "ggridges", "globals", "gplots", "Hmisc", "hms", "htmlTable", "huge", "igraph", "jpeg", "jsonlite", "KernSmooth", "knitr", "lattice", "latticeExtra", "lifecycle", "listenv", "lmerTest", "loo", "MASS", "Matrix", "mcmc", "MCMCpack", "mgcv", "mime", "mnormt", "modeltools", "MuMIn", "mvtnorm", "nlme", "nloptr", "nnet", "pillar", "plyr", "prettyunits", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantreg", "R6", "Rcpp", "RcppEigen", "rlang", "rmarkdown", "rsconnect", "RSQLite", "rstan", "rstanarm", "rstudioapi", "scales", "shinyjs", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tidyr", "tidyselect", "tinytex", "TSP", "vctrs", "xfun", "xts", "yaml", "zoo"))
install.packages(c("asbio", "bayesplot", "BDgraph", "BH", "bit", "blavaan", "blob", "boot", "broom", "callr", "caTools", "checkmate", "cli", "curl", "data.table", "DBI", "dendextend", "DescTools", "deSolve", "digest", "dplyr", "DT", "emmeans", "fansi", "forcats", "foreach", "foreign", "future", "future.apply", "ggm", "ggplot2", "ggridges", "globals", "gplots", "Hmisc", "hms", "htmlTable", "huge", "igraph", "jpeg", "jsonlite", "KernSmooth", "knitr", "lattice", "latticeExtra", "lifecycle", "listenv", "lmerTest", "loo", "MASS", "Matrix", "mcmc", "MCMCpack", "mgcv", "mime", "mnormt", "modeltools", "MuMIn", "mvtnorm", "nlme", "nloptr", "nnet", "pillar", "plyr", "prettyunits", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantreg", "R6", "Rcpp", "RcppEigen", "rlang", "rmarkdown", "rsconnect", "RSQLite", "rstan", "rstanarm", "rstudioapi", "scales", "shinyjs", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tidyr", "tidyselect", "tinytex", "TSP", "vctrs", "xfun", "xts", "yaml", "zoo"))
install.packages(c("asbio", "bayesplot", "BDgraph", "BH", "bit", "blavaan", "blob", "boot", "broom", "callr", "caTools", "checkmate", "cli", "curl", "data.table", "DBI", "dendextend", "DescTools", "deSolve", "digest", "dplyr", "DT", "emmeans", "fansi", "forcats", "foreach", "foreign", "future", "future.apply", "ggm", "ggplot2", "ggridges", "globals", "gplots", "Hmisc", "hms", "htmlTable", "huge", "igraph", "jpeg", "jsonlite", "KernSmooth", "knitr", "lattice", "latticeExtra", "lifecycle", "listenv", "lmerTest", "loo", "MASS", "Matrix", "mcmc", "MCMCpack", "mgcv", "mime", "mnormt", "modeltools", "MuMIn", "mvtnorm", "nlme", "nloptr", "nnet", "pillar", "plyr", "prettyunits", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantreg", "R6", "Rcpp", "RcppEigen", "rlang", "rmarkdown", "rsconnect", "RSQLite", "rstan", "rstanarm", "rstudioapi", "scales", "shinyjs", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tidyr", "tidyselect", "tinytex", "TSP", "vctrs", "xfun", "xts", "yaml", "zoo"))
install.packages(c("asbio", "bayesplot", "BDgraph", "BH", "bit", "blavaan", "blob", "boot", "broom", "callr", "caTools", "checkmate", "cli", "curl", "data.table", "DBI", "dendextend", "DescTools", "deSolve", "digest", "dplyr", "DT", "emmeans", "fansi", "forcats", "foreach", "foreign", "future", "future.apply", "ggm", "ggplot2", "ggridges", "globals", "gplots", "Hmisc", "hms", "htmlTable", "huge", "igraph", "jpeg", "jsonlite", "KernSmooth", "knitr", "lattice", "latticeExtra", "lifecycle", "listenv", "lmerTest", "loo", "MASS", "Matrix", "mcmc", "MCMCpack", "mgcv", "mime", "mnormt", "modeltools", "MuMIn", "mvtnorm", "nlme", "nloptr", "nnet", "pillar", "plyr", "prettyunits", "processx", "ps", "psych", "psycho", "purrr", "qgraph", "quantreg", "R6", "Rcpp", "RcppEigen", "rlang", "rmarkdown", "rsconnect", "RSQLite", "rstan", "rstanarm", "rstudioapi", "scales", "shinyjs", "SparseM", "StanHeaders", "stringi", "survival", "threejs", "tidyr", "tidyselect", "tinytex", "TSP", "vctrs", "xfun", "xts", "yaml", "zoo"))
help(package)
help(data)
installed.packages()
install.packages(IIT-package)
install.packages(IIS-package)
install.packages("IIS-package")
install.packages("IIS")
installed.packages()
data("IIS")
data("pines_1997")
library(IIS)
install.packages("Stat2Data")
library(Stat2Data)
data(pines)
data("pines")
data(Pines)
summary(Pines)
install.packages(IIS-package)
load("/Users/yingxuan/Downloads/IIS/data/pines_1997.rda")
View(pines_1997)
install.packages("~/Downloads/IIS_1.0.tar.gz", repos = NULL, type = "source")
a <- 1:10/5
print(a)
a = 1:(10/5)
print (a)
for (i in 1:5) {b <- 2^i; print(b)}
install.packages("shiny")
1-pchisq(2(7-1), 7)
1-pchisq(2*(7-1), 7)
1-pchisq(2*(7-1), 6)
1-pchisq(2*(8-1), 6)
1-pchisq(2*(8-1), 7)
1-pchisq(2*(17-1), 16)
1-pchisq(2*20, 20)
qt(0.95, df=99)
qt(0.95, df=100)
qt(0.9725, df=99)
qt(0.9725, df=100)
qt(0.025, df=99)
qt(p=(1-0.025), df=99)
qt(p=0.05, df=99)
qt(p=(1-0.05), df=99)
qt(p=(1-0.025), df=99)
qt(p=(1-0.025), df=9)
qchisq(p=0.05, 99)
qchisq(p=0.95, df=99)
qchisq(p=0.95, df=99)
qnorm(0.95)
qnorm(0.975)
qt(p=0.95, df=538)
qt(p=0.975, df=538)
qt(p=0.95, df=539)
qnorm(0.95)
remove(list = ls())
qqline(data, col=2)
library(grid)
library(lattice)
library(plotrix)
data <- c(24.00, 28.00, 27.75, 27.00, 24.25, 23.50, 26.25, 24.00, 25.00, 30.00,23.25, 26.25, 21.50, 26.00, 28.00, 24.50, 22.50, 28.25, 21.25, 19.75)
qqnorm(data)
qqline(data, col=2)
mean(data)
var(data)
qt(p=0.0975, df=19)
qt(p=0.975, df=19)
qt(p=0.975, df=99)
dataX <- c(12.0129, 12.0072, 12.0064, 12.0054, 12.0016,11.9853, 11.9949, 11.9985, 12.0077, 12.0061)
dataY <- c(12.0318, 12.0246, 12.0069, 12.0006, 12.0075)
mean(dataX)
mean(dataY)
var(dataX)
var(dataY)
qt(p=0.975, df=23)
qt(p=0.95, df=23)
qt(p=0.975, df=13)
qt(p=0.975, df=4)
qt(p=0.975, df=4.48)
qt(p=0.975, df=5.48)
qt(p=0.975, df=5)
qf(p=0.95, df1 = 8, df2= 8)
## Including libraries
remove(list=objects())
## Including libraries
remove(list=objects())
####### Lecture 2-2
## Facebook friends
facfr<-c(108, 103, 352, 121, 93, 53, 40, 53, 22, 116, 94)
facor<-facfr[order(facfr,decreasing=F)]
mean(facor)
median(facor)
facornew<-facor[-11]
mean(facornew)
median(facornew)
summary(facor)
### Morley data set
speed<-morley[,3]
mean(speed)
sd(speed)
summary(speed)
boxplot(speed)
hist(speed)
hist(speed, breaks=20)
### qq plot
qqnorm(speed)
qqline(speed,col=2)
dat1<-rnorm(2000)
qqnorm(dat1);
qqline(dat1,col=2)
dat2<-rexp(200)
qqnorm(dat2);
qqline(dat2,col=2)
dat3<-rpois(2000,200)
qqnorm(dat3);
qqline(dat3,col=2)
dat4<-2*((runif(1000)>0.5)-0.5)*(runif(1000))^(-0.5)
qqnorm(dat4,ylim=c(-10,10));
qqline(dat4,col=2)
### scatter plot: 31 Black cherry trees. girth (diameter), height,  volume
plot(trees[,1],trees[,2])
splom(trees)
### 5 Orange trees circumference at age
plot(Orange[,2],Orange[,3])
xyplot(circumference~age,Orange,groups= Tree,cex=1,pch=c(1,2,3,4,5))
### Anscombe look for help(anscombe)
summary(anscombe)
boxplot(anscombe)
##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
for(i in 1:4) {
ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
## or   ff[[2]] <- as.name(paste0("y", i))
##      ff[[3]] <- as.name(paste0("x", i))
mods[[i]] <- lmi <- lm(ff, data = anscombe)
print(anova(lmi))
}
## See how close they are (numerically!)
sapply(mods, coef)
lapply(mods, function(fm) coef(summary(fm)))
## Now, do what you should have done in the first place: PLOTS
op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))
for(i in 1:4) {
ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
xlim = c(3, 19), ylim = c(3, 13))
abline(mods[[i]], col = "blue")
}
mtext("Anscombe's 4 Regression data sets", outer = TRUE, cex = 1.5)
par(op)
# Remove all variables from the R environment to create a fresh start
rm(list=ls())
setwd("~/Documents/SUTD/Term 5/ESA/Week 9/Lecture 15")
# Set the working folder -- FILL IN THE LINE BELOW
setwd("~/Documents/SUTD/Term 5/ESA/Week 9/Lecture 15")
# Import data from EngineDesign.csv
mydata <- read.csv(file="EngineDesign.csv", head=TRUE)
View(mydata)
# Determine the 'best' solution according to the Lexicographic method
# -> we simply have to look for the minimum (or maximum) value of each objective -- FILL IN THE LINES BELOW
# Horsepower (to be maximized)
max(mydata[,1]) # ...
which.max(mydata[,1]) # ...
# Cost (to be minimized)
min(mydata[,2]) # ...
which.min(mydata[,2]) # ...
# Fuel.efficiency (to be maximized)
max(mydata[,3]) # ...
which.max(mydata[,3]) # 4
# First, we identify the Pareto-efficient solutions (we want to carry out the analysis only for the non-dominated solution)
input <- matrix(c(mydata$Horsepower,mydata$Cost*(-1),mydata$Fuel.efficiency),nrow=43,ncol=3)
source("ParetoSorting_adv.R")
result <- ParetoSorting_adv(input,"MAX") # We use the ParetoSorting_adv function to identify the non-dominated solutions (denoted with 0)
index <- which(result==0) # We store in a vector the index of the Pareto-efficient solutions
# Second, we find the coordinates of the Ideal (Utopia) and Nadir objective vector
# (we look for the minimum and maximum value of the objective functions only for the efficient solutions)
Utopia <- c(max(input[index,1]),max(input[index,2]),max(input[index,3])) # 0.679458 -0.014658  0.744870
Nadir  <- c(min(input[index,1]),min(input[index,2]),min(input[index,3])) # 0.348350 -0.162701  0.709748
# Third, we normalize the data (since the objectives have a different range of variation). Let's normalize only the Pareto-efficient solutions -- FILL IN THE LINES BELOW
obj1_norm <- (input[index,1]-min(input[index,1]))/(max(input[index,1])-min(input[index,1]))
obj2_norm <- (input[index,2]-min(input[index,2]))/(max(input[index,2])-min(input[index,2]))
obj3_norm <- (input[index,3]-min(input[index,3]))/(max(input[index,3])-min(input[index,3]))
# The coordinates of the Utopia point for the normalised data are 1 1 1. Let's double-check this ...
Utopia_norm <- c(max(obj1_norm),max(obj2_norm),max(obj3_norm)) # 1 1 1
# We calculate the distance between each solution and the Utopia point, and we find the point with the minimum distance
# We have to do this only for the Pareto-efficient solutions -- FILL IN THE LINES BELOW
Distance <- rep(0,times=40)
for (i in 1:40){
Distance[i] <- sqrt((obj1_norm[i] - Utopia_norm[1])^2 + (obj2_norm[i] - Utopia_norm[2])^2 + (obj3_norm[i] - Utopia_norm[3])^2)
}
# plot(Distance)
min(Distance) # ...
which.min(Distance) # ...
index[28] # closet solution to the Utopia point
# Finally, Let's visualize this solution, along with the set of all solutions and the Utopia point -- FILL IN THE LINES BELOW
#
# Load ggplot2
library(ggplot2)
#
# Create data frames for the visualization
newdata <- data.frame(Horsepower=obj1_norm,Cost=obj2_norm,Fuel.efficiency=obj3_norm)
newdata_utopia <- data.frame(Horsepower=1,Cost=1,Fuel.efficiency=1)
newdata_sel_point <- data.frame(Horsepower=obj1_norm[28],Cost=obj2_norm[28],Fuel.efficiency=obj3_norm[28])
newdata$cat <- "Pareto-eff. sol."
newdata_utopia$cat <- "Utopia point"
newdata_sel_point$cat <- "Selected sol."
df <- rbind(newdata, newdata_utopia, newdata_sel_point)
#
# Plot
ggplot(data = df,
mapping = aes(x=Horsepower, y = Cost, size = Fuel.efficiency)) +
geom_point(aes(colour=cat)) +
coord_cartesian(xlim=c(0,1), ylim=c(0,1)) +
labs(x="Horsepower", y="Cost")
index[28] # closet solution to the Utopia point: value of 30
?index
??index
plot(Distance)
index
